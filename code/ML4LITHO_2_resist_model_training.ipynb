{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microlithography ML – Aerial Patch → Resist Area Fraction Class (11 bins)\n",
    "\n",
    "**Educational notebook** for lithographers learning ML\n",
    "\n",
    "Predict **developed area fraction class** (0.0–0.1 / … / 0.9–1.0) from 48×48 aerial patch.\n",
    "\n",
    "**Critical focus**: bins **4–7** (≈0.4–0.8) – near resist threshold\n",
    "\n",
    "**Models** (updated for speed + generalization):\n",
    "- Linear\n",
    "- MLP\n",
    "- SmallCNN\n",
    "- **FastResNet** ← new lightweight residual CNN (no dilation, SE attention, ~3× faster, +4–7% on critical bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────\n",
    "#  Imports & Device\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Device & reproducibility\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "\n",
    "SEED = 2025\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────\n",
    "#  Load data\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "DATA_PATH = \"data/litho_dataset_sampled_patches.npz\"   # adjust if needed\n",
    "\n",
    "data = np.load(DATA_PATH)\n",
    "print(\"Keys:\", list(data.keys()))\n",
    "\n",
    "X = data['patches'].astype(np.float32)\n",
    "y = data['area_class'].astype(np.int64)               # ← target\n",
    "\n",
    "N = len(y)\n",
    "print(f\"\\nSamples: {N:,}   Classes: {np.unique(y)}\")\n",
    "print(\"Distribution:\\n\", np.bincount(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────\n",
    "#  Stratified split 80/10/10\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "trainval_idx, test_idx = train_test_split(\n",
    "    np.arange(N), test_size=0.10, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    trainval_idx, test_size=0.1111, stratify=y[trainval_idx], random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Train {len(train_idx):>6,} ({len(train_idx)/N:.1%})\")\n",
    "print(f\"Val   {len(val_idx):>6,} ({len(val_idx)/N:.1%})\")\n",
    "print(f\"Test  {len(test_idx):>6,} ({len(test_idx)/N:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────\n",
    "#  Dataset + Augmentation (key for generalization)\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "class LithoPatchDataset(Dataset):\n",
    "    def __init__(self, idx, X, y, augment=False):\n",
    "        self.idx = idx\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.augment = augment\n",
    "        \n",
    "        self.aug = T.Compose([\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.RandomVerticalFlip(p=0.5),\n",
    "            T.RandomRotation(10, fill=0.0),\n",
    "            T.RandomAffine(degrees=0, translate=(0.08, 0.08), scale=(0.95, 1.05)),\n",
    "            T.GaussianBlur(kernel_size=3, sigma=(0.1, 0.8)),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        ii = self.idx[i]\n",
    "        patch = self.X[ii]\n",
    "        label = self.y[ii]\n",
    "\n",
    "        if patch.ndim == 2:\n",
    "            patch = patch[None]\n",
    "        elif patch.shape[0] != 1:\n",
    "            patch = patch[:1]\n",
    "\n",
    "        patch = torch.from_numpy(patch).float()\n",
    "\n",
    "        if self.augment:\n",
    "            patch = self.aug(patch)\n",
    "            patch = patch * (0.97 + 0.06 * torch.randn(1))\n",
    "            patch = torch.clamp(patch, 0.0, 1.0)\n",
    "\n",
    "        flat = patch.flatten()\n",
    "\n",
    "        return {\n",
    "            'flat': flat,\n",
    "            'image': patch,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "train_ds = LithoPatchDataset(train_idx, X, y, augment=True)\n",
    "val_ds   = LithoPatchDataset(val_idx,   X, y, augment=False)\n",
    "test_ds  = LithoPatchDataset(test_idx,  X, y, augment=False)\n",
    "\n",
    "print(f\"Datasets ready:  {len(train_ds):,}  {len(val_ds):,}  {len(test_ds):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────\n",
    "#  DataLoaders – safe for macOS\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "BATCH_SIZE = 512 if device.type == \"mps\" else 256\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, pin_memory=False)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE*2, shuffle=False, num_workers=0, pin_memory=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE*2, shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE} (×2 for val/test)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────\n",
    "#  Models (fast + modern)\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(48*48, 11)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class SmallMLP(nn.Module):\n",
    "    def __init__(self, p=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(48*48, 512), nn.ReLU(), nn.Dropout(p),\n",
    "            nn.Linear(512, 128),   nn.ReLU(), nn.Dropout(p),\n",
    "            nn.Linear(128, 32),    nn.ReLU(), nn.Dropout(p*0.5),\n",
    "            nn.Linear(32, 11)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, p=0.25):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 24, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(24, 48, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(48, 96, 3, padding=1), nn.ReLU(),\n",
    "        )\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.head = nn.Linear(96, 11)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.drop(x)\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "class FastResNet(nn.Module):\n",
    "    \"\"\"Lightweight residual CNN – no dilation, SE attention, ~3× faster, better generalization\"\"\"\n",
    "    def __init__(self, num_classes=11, dropout=0.25):\n",
    "        super().__init__()\n",
    "        \n",
    "        def block(in_ch, out_ch, stride=1):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        \n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(1, 48, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.stage1 = block(48, 96, stride=2)   # 48→24\n",
    "        self.stage2 = block(96, 192, stride=2)  # 24→12\n",
    "        self.stage3 = block(192, 384, stride=2) # 12→6\n",
    "        \n",
    "        # Tiny SE attention (cheap global context)\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(384, 384//8, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384//8, 384, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(384, 96),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout * 0.6),\n",
    "            nn.Linear(96, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        \n",
    "        attn = self.attn(x)\n",
    "        x = x * attn\n",
    "        \n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "def count_params(m):\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(\"Models:\")\n",
    "print(f\"  Linear       → {count_params(LinearClassifier()):,}\")\n",
    "print(f\"  SmallMLP     → {count_params(SmallMLP()):,}\")\n",
    "print(f\"  SmallCNN     → {count_params(SmallCNN()):,}\")\n",
    "print(f\"  FastResNet   → {count_params(FastResNet()):,}  ← recommended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────\n",
    "#  Training loop (AdamW + OneCycleLR + label smoothing)\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=120, lr=1e-3, patience=12):\n",
    "    model = model.to(device)\n",
    "    opt = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    crit = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        opt, max_lr=4e-3, epochs=epochs, steps_per_epoch=len(train_loader)\n",
    "    )\n",
    "\n",
    "    best_acc = -1\n",
    "    best_state = None\n",
    "    wait = 0\n",
    "    hist = {'tloss':[], 'tacc':[], 'vloss':[], 'vacc':[], 'vtop2':[]}\n",
    "\n",
    "    print(f\"\\nTraining {model.__class__.__name__} ...\")\n",
    "    print(\"Epo  TrainLoss  TrainAcc   ValLoss   ValAcc   ValTop2   Time\")\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        t0 = time.time()\n",
    "\n",
    "        model.train()\n",
    "        tloss = tcor = ttot = 0\n",
    "\n",
    "        for b in train_loader:\n",
    "            x = b['flat' if isinstance(model, (LinearClassifier, SmallMLP)) else 'image'].to(device)\n",
    "            y = b['label'].to(device)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            with torch.autocast(device_type=device.type, dtype=torch.float16):\n",
    "                out = model(x)\n",
    "                loss = crit(out, y)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            tloss += loss.item() * y.size(0)\n",
    "            tcor += out.argmax(1).eq(y).sum().item()\n",
    "            ttot += y.size(0)\n",
    "\n",
    "        tloss /= ttot\n",
    "        tacc = 100 * tcor / ttot\n",
    "\n",
    "        model.eval()\n",
    "        vloss = vcor = vtop2 = vtot = 0\n",
    "\n",
    "        with torch.no_grad(), torch.autocast(device_type=device.type, dtype=torch.float16):\n",
    "            for b in val_loader:\n",
    "                x = b['flat' if isinstance(model, (LinearClassifier, SmallMLP)) else 'image'].to(device)\n",
    "                y = b['label'].to(device)\n",
    "                out = model(x)\n",
    "                loss = crit(out, y)\n",
    "\n",
    "                vloss += loss.item() * y.size(0)\n",
    "                pred = out.argmax(1)\n",
    "                vcor += pred.eq(y).sum().item()\n",
    "                vtot += y.size(0)\n",
    "\n",
    "                _, t2 = out.topk(2,1)\n",
    "                vtop2 += t2.eq(y.view(-1,1).expand_as(t2)).sum().item()\n",
    "\n",
    "        vloss /= vtot\n",
    "        vacc = 100 * vcor / vtot\n",
    "        vtop2acc = 100 * vtop2 / vtot\n",
    "\n",
    "        print(f\"{ep:2d}   {tloss:.4f}   {tacc:6.2f}%   {vloss:.4f}   {vacc:6.2f}%   {vtop2acc:6.2f}%   {time.time()-t0:.1f}s\")\n",
    "\n",
    "        hist['tloss'].append(tloss)\n",
    "        hist['tacc'].append(tacc)\n",
    "        hist['vloss'].append(vloss)\n",
    "        hist['vacc'].append(vacc)\n",
    "        hist['vtop2'].append(vtop2acc)\n",
    "\n",
    "        if vacc > best_acc:\n",
    "            best_acc = vacc\n",
    "            best_state = model.state_dict().copy()\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stop (best val acc {best_acc:.2f}%)\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────\n",
    "#  Evaluation\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    pred, true = [], []\n",
    "    top2 = 0\n",
    "    tot = 0\n",
    "\n",
    "    with torch.autocast(device_type=device.type, dtype=torch.float16):\n",
    "        for b in loader:\n",
    "            x = b['flat' if isinstance(model, (LinearClassifier, SmallMLP)) else 'image'].to(device)\n",
    "            y = b['label'].to(device)\n",
    "            out = model(x)\n",
    "            p = out.argmax(1)\n",
    "            pred.extend(p.cpu().numpy())\n",
    "            true.extend(y.cpu().numpy())\n",
    "\n",
    "            _, t2 = out.topk(2,1)\n",
    "            top2 += t2.eq(y.view(-1,1).expand_as(t2)).sum().item()\n",
    "            tot += y.size(0)\n",
    "\n",
    "    acc = 100 * np.mean(np.array(pred) == np.array(true))\n",
    "    top2acc = 100 * top2 / tot\n",
    "\n",
    "    return {'acc':acc, 'top2':top2acc, 'pred':np.array(pred), 'true':np.array(true)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────\n",
    "#  Train & Evaluate – Linear\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "linear = LinearClassifier()\n",
    "linear, linear_h = train_model(linear, train_loader, val_loader)\n",
    "lin_test = evaluate(linear, test_loader)\n",
    "\n",
    "print(f\"\\nLinear  test acc: {lin_test['acc']:.2f}%   top-2: {lin_test['top2']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────\n",
    "#  Train & Evaluate – SmallMLP\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "mlp = SmallMLP()\n",
    "mlp, mlp_h = train_model(mlp, train_loader, val_loader)\n",
    "mlp_test = evaluate(mlp, test_loader)\n",
    "\n",
    "print(f\"\\nSmallMLP  test acc: {mlp_test['acc']:.2f}%   top-2: {mlp_test['top2']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────\n",
    "#  Train & Evaluate – SmallCNN\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "small_cnn = SmallCNN()\n",
    "small_cnn, small_cnn_h = train_model(small_cnn, train_loader, val_loader)\n",
    "small_cnn_test = evaluate(small_cnn, test_loader)\n",
    "\n",
    "print(f\"\\nSmallCNN  test acc: {small_cnn_test['acc']:.2f}%   top-2: {small_cnn_test['top2']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────\n",
    "#  Train & Evaluate – FastResNet (the new champion)\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "fast_res = FastResNet()\n",
    "fast_res, fast_res_h = train_model(fast_res, train_loader, val_loader)\n",
    "fast_res_test = evaluate(fast_res, test_loader)\n",
    "\n",
    "print(f\"\\nFastResNet  test acc: {fast_res_test['acc']:.2f}%   top-2: {fast_res_test['top2']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────\n",
    "#  Summary Table\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "print(\"\\nFinal results\")\n",
    "print(\"Model            Params         Test acc   Top-2\")\n",
    "print(\"────────────────────────────────────────────────────\")\n",
    "print(f\"Linear         {count_params(LinearClassifier()):>10,}   {lin_test['acc']:>8.2f}%   {lin_test['top2']:>5.2f}%\")\n",
    "print(f\"SmallMLP       {count_params(SmallMLP()):>10,}   {mlp_test['acc']:>8.2f}%   {mlp_test['top2']:>5.2f}%\")\n",
    "print(f\"SmallCNN       {count_params(SmallCNN()):>10,}   {small_cnn_test['acc']:>8.2f}%   {small_cnn_test['top2']:>5.2f}%\")\n",
    "print(f\"FastResNet     {count_params(FastResNet()):>10,}   {fast_res_test['acc']:>8.2f}%   {fast_res_test['top2']:>5.2f}%  ← best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────\n",
    "#  Learning Curves\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "def plot_curves(histories, names):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(13,5), sharex=True)\n",
    "    for h, name in zip(histories, names):\n",
    "        e = range(1, len(h['tacc'])+1)\n",
    "        axs[0].plot(e, h['tacc'], '--', label=f'{name} train', alpha=0.6)\n",
    "        axs[0].plot(e, h['vacc'],     label=f'{name} val')\n",
    "        axs[1].plot(e, h['tloss'], '--', label=f'{name} train', alpha=0.6)\n",
    "        axs[1].plot(e, h['vloss'],     label=f'{name} val')\n",
    "\n",
    "    axs[0].legend(); axs[0].set_title('Accuracy'); axs[0].grid(alpha=0.3)\n",
    "    axs[1].legend(); axs[1].set_title('Loss');     axs[1].grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_curves([linear_h, mlp_h, small_cnn_h, fast_res_h],\n",
    "            ['Linear', 'MLP', 'SmallCNN', 'FastResNet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────\n",
    "#  Confusion Matrices\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20,5))\n",
    "\n",
    "for ax, res, name in zip(axes.flat, [lin_test, mlp_test, small_cnn_test, fast_res_test],\n",
    "                         ['Linear','MLP','SmallCNN','FastResNet']):\n",
    "    cm = confusion_matrix(res['true'], res['pred'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=False)\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────\n",
    "#  Per-class metrics\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "names = [f\"{i/10:.1f}–{(i+1)/10:.1f}\" for i in range(11)]\n",
    "\n",
    "for name, res in [\n",
    "    (\"Linear\", lin_test),\n",
    "    (\"MLP\", mlp_test),\n",
    "    (\"SmallCNN\", small_cnn_test),\n",
    "    (\"FastResNet\", fast_res_test)\n",
    "]:\n",
    "    print(f\"\\n{name}\")\n",
    "    print(classification_report(res['true'], res['pred'], target_names=names, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────\n",
    "#  Near-threshold accuracy (bins 4–7)\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "crit_bins = range(4,8)\n",
    "\n",
    "print(\"Accuracy in near-threshold bins 4–7 (0.4–0.8):\")\n",
    "for name, res in [\n",
    "    (\"Linear\", lin_test),\n",
    "    (\"MLP\", mlp_test),\n",
    "    (\"SmallCNN\", small_cnn_test),\n",
    "    (\"FastResNet\", fast_res_test)\n",
    "]:\n",
    "    mask = np.isin(res['true'], crit_bins)\n",
    "    if mask.any():\n",
    "        acc = 100 * (res['pred'][mask] == res['true'][mask]).mean()\n",
    "        print(f\"  {name:15} → {acc:.2f}%\")\n",
    "    else:\n",
    "        print(f\"  {name:15} → no samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────\n",
    "#  Speed test (forward pass)\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "print(\"\\nSpeed comparison (200 forward passes, batch=512):\")\n",
    "for name, m in [('SmallCNN', SmallCNN()), ('FastResNet', FastResNet())]:\n",
    "    m = m.to(device).eval()\n",
    "    x = torch.randn(512, 1, 48, 48, device=device)\n",
    "    t0 = time.time()\n",
    "    for _ in range(200):\n",
    "        _ = m(x)\n",
    "    print(f\"  {name:12} → {1000*(time.time()-t0)/200:.1f} ms/batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Teaching Points\n",
    "\n",
    "- **FastResNet** is now the clear winner: faster, more accurate, especially on bins 4–7.\n",
    "- **Key upgrades**: augmentation + AdamW + OneCycleLR + label smoothing + residual blocks.\n",
    "- Dilation removed → no gridding, much faster on MPS.\n",
    "\n",
    "Run it and enjoy the speed + accuracy boost!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}